{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create training and test dataloaders\n",
    "\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8f356b9eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the NN architecture\n",
    "class ConvAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # 28 -> 14\n",
    "        self.encoder1 = torch.nn.Sequential(torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # 14 -> 7\n",
    "        self.encoder2 = torch.nn.Sequential(torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # 7 -> 3\n",
    "        self.encoder3 = torch.nn.Sequential(torch.nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=2, dilation=1),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.MaxPool2d(kernel_size=3, stride=3))\n",
    "        # 3 -> 1\n",
    "        self.encoder4 = torch.nn.Sequential(torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # ( 28 > 14 2*) ( 14 > 7 2*) (7+2 > 3) (3 > 1)\n",
    "        ### decoder layers ###\n",
    "        # 1 -> 3\n",
    "        self.decoder4 = torch.nn.Sequential(torch.nn.ConvTranspose2d(64, 32, kernel_size=3, stride=3),\n",
    "                                            torch.nn.ReLU())\n",
    "\n",
    "        # 3 -> 7\n",
    "        self.decoder3 = torch.nn.Sequential(torch.nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2),\n",
    "                                            torch.nn.ReLU())\n",
    "        # 7 -> 14\n",
    "        self.decoder2 = torch.nn.Sequential(torch.nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "                                            torch.nn.ReLU())\n",
    "        # 14 -> 28\n",
    "        self.decoder1 = torch.nn.Sequential(torch.nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "                                            torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### encode ###\n",
    "        #print(x.shape)\n",
    "        x = self.encoder1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.encoder2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.encoder3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.encoder4(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        ### decode ###\n",
    "        x = self.decoder4(x)\n",
    "        #print(x.shape)\n",
    "        x = self.decoder3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.decoder2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.decoder1(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        ### x expected to be 7*7*16 ###\n",
    "        ### x expected to be 1*1*10 ###\n",
    "        x = self.decoder4(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder1(x)\n",
    "        return x\n",
    "        \n",
    "    def decode_last(self, x):\n",
    "        ### x expected to be 7*7*16 ###\n",
    "        x = self.decoder1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (encoder1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (encoder2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (encoder3): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (encoder4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder4): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder3): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder2): Sequential(\n",
      "    (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder1): Sequential(\n",
      "    (0): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### library for plot online update\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAE1CAYAAAB3OO7MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAd2klEQVR4nO3df7RXdZ3v8edbQBhQEREbFAscnZWAeMAz6GSZhDbQuoUZKWSOODk2dL2zmu446fTDtGZudk0d11gzWHatuCrRdaI7/hpLKu8y82CE4I9ExPGE6QEVNfwFvu8f3y19OR7O+R44nHPg83ys9V1n7/157/39fLbgi8/e3/PdkZlIklSSvfq6A5Ik9TbDT5JUHMNPklQcw0+SVBzDT5JUHMNPklQcw0/qRRGxNiJO2gXHXRoR5/T0caU9leEnSSqO4SdJKo7hJ/WRiBgcEVdGxLrqdWVEDK5r/7uIeLJqOyciMiIOb+C4e0XEZyPi8Yh4OiK+HRHDq7YhEfHdiNgQEc9FxL0R8ZaqbV5ErImIFyLisYg4Y9eNXupbhp/Udz4DHAc0AUcDU4HPAkTEDOBTwEnA4cC7u3HcedVrGnAYsA/wz1XbWcBw4FBgJPBXwEsRMQy4CpiZmfsC7wCW7/DIpH7O8JP6zhnAJZn5dGa2ARcDZ1ZtpwHfysxVmbmpauvOcS/PzDWZ+SJwITAnIgYCr1ELvcMzc0tmLsvM56v9XgcmRsQfZOaTmbmqB8Yo9UuGn9R3DgYer1t/vNr2RtsTdW31yzty3IHAW4DvALcBN1SXU78SEYMy83fA6dRmgk9GxL9HxNu7NRppN2L4SX1nHfC2uvW3VtsAngTG1LUdupPH3Qw8lZmvZebFmTme2qXN/wL8OUBm3paZJwOjgYeAa7rxntJuxfCT+s71wGcjYlREHAh8Hvhu1bYIODsijoyIoVVbd477NxExLiL2Af4RuDEzN0fEtIg4KiIGAM9Tuwy6JSLeEhEfqO79vQK8CGzpmWFK/Y/hJ/WdLwEtwArgfuC+ahuZeQu1D6DcCawG7q72eaWB415L7fLmT4HHgJeB/1a1/SGwmFrwPQj8hFrg7gX8d2qzxmeofcDmEzszOKk/Cx9mK/V/EXEksBIYnJmb+7o/0u7OmZ/UT0XEByNi74gYAVwK/NDgk3qG4Sf1Xx8H2oBHqd1/m9+33ZH2HF72lCQVx5mfJKk4A/u6A90xY8aMXL9+fV93Q5K0m1i2bNltmTmj/fbdKvwAWlpa+roLkqTdRER0uH23uuzprE+S1E0HdrRxtwo/SZJ6guEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKk5D4RcRMyLi4YhYHREXdNA+OCJurNrviYix1fYzImJ53ev1iGiq2uZGxP0RsSIibo2IA3tyYJIkbU+X4RcRA4CrgZnAeGBuRIxvV/Yx4NnMPBy4ArgUIDMXZmZTZjYBZwJrM3N5RAwE/gmYlpmTgBXAeT01KEmSOtPIzG8qsDoz12Tmq8ANwKx2NbOA66rlxcD0iIh2NXOB66vlqF7Dqrr9gHU70H9JkrqtkfA7BHiibr212tZhTWZuBjYCI9vVnE4Vfpn5GjAfuJ9a6I0HvtnRm0fEuRHREhEtbW1tDXRXkqTONRJ+7WdwANmdmog4FtiUmSur9UHUwm8ycDC1y54XdvTmmbkgM5szs3nUqFENdFeSpM41En6twKF162N48yXKrTXV/bzhwDN17XP4/SVPgCaAzHw0MxNYBLyjWz2XJGkHNRJ+9wJHRMS4iNibWpAtaVezBDirWp4N/LgKNSJiL+DD1O4VvuE3wPiIeGMqdzLw4I4NQZKk7hnYVUFmbo6I84DbgAHAtZm5KiIuAVoycwm1+3XfiYjV1GZ8c+oOcQLQmplr6o65LiIuBn4aEa8BjwPzempQkiR1JqoJ2m6hubk5W1pa+robkqTdREQsy8zm9tv9hhdJUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEMP0lScQw/SVJxDD9JUnEaCr+ImBERD0fE6oi4oIP2wRFxY9V+T0SMrbafERHL616vR0RT1bZ3RCyIiF9HxEMR8aGeHJgkSdvTZfhFxADgamAmMB6YGxHj25V9DHg2Mw8HrgAuBcjMhZnZlJlNwJnA2sxcXu3zGeDpzPzj6rg/6YkBSZLUlUZmflOB1Zm5JjNfBW4AZrWrmQVcVy0vBqZHRLSrmQtcX7f+F8D/AMjM1zNzfXc7L0nSjmgk/A4Bnqhbb622dViTmZuBjcDIdjWnU4VfROxfbftiRNwXEd+LiLd09OYRcW5EtERES1tbWwPdlSSpc42EX/sZHEB2pyYijgU2ZebKatNAYAzw/zJzCnA3cFlHb56ZCzKzOTObR40a1UB3JUnqXCPh1wocWrc+Bli3vZqIGAgMB56pa5/Dtpc8NwCbgJuq9e8BUxrutSRJO6GR8LsXOCIixkXE3tSCbEm7miXAWdXybODHmZkAEbEX8GFq9woBqNp+CJxYbZoOPLCDY5AkqVsGdlWQmZsj4jzgNmAAcG1mroqIS4CWzFwCfBP4TkSspjbjm1N3iBOA1sxc0+7Qn672uRJoA87e+eFIktS1qCZou4Xm5uZsaWnp625IknYTEbEsM5vbb/cbXiRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxWko/CJiRkQ8HBGrI+KCDtoHR8SNVfs9ETG22n5GRCyve70eEU3t9l0SESt7YjCSJDWiy/CLiAHA1cBMYDwwNyLGtyv7GPBsZh4OXAFcCpCZCzOzKTObgDOBtZm5vO7YpwIv9shIJElqUCMzv6nA6sxck5mvAjcAs9rVzAKuq5YXA9MjItrVzAWuf2MlIvYBPgV8aUc6LknSjmok/A4Bnqhbb622dViTmZuBjcDIdjWnUxd+wBeBrwKbOnvziDg3IloioqWtra2B7kqS1LmBDdS0n8EBZHdqIuJYYFNmrqzWm4DDM/Nv3rg/uD2ZuQBYANDc3Nz+fSVpt/Daa6/R2trKyy+/3Ndd2SMNGTKEMWPGMGjQoIbqGwm/VuDQuvUxwLrt1LRGxEBgOPBMXfsctp31/SlwTESsrfpwUEQszcwTG+q1JO1mWltb2XfffRk7dixvviuknZGZbNiwgdbWVsaNG9fQPo1c9rwXOCIixkXE3tSCbEm7miXAWdXybODHmZkAEbEX8GFq9wrf6OjXM/PgzBwLvBP4tcEnaU/28ssvM3LkSINvF4gIRo4c2a1ZdZczv8zcHBHnAbcBA4BrM3NVRFwCtGTmEuCbwHciYjW1Gd+cukOcALRm5ppujEWS9jgG367T3XPbyGVPMvNm4OZ22z5ft/wytdldR/suBY7r5NhrgYmN9EOSpJ7gN7xIUgGee+45vva1r+3Qvu973/t47rnnOq35/Oc/zx133LFDx+8Lhp8kFaCz8NuyZUun+958883sv//+ndZccsklnHTSSTvcv95m+ElSAS644AIeffRRmpqaOP/881m6dCnTpk3jIx/5CEcddRQAp5xyCscccwwTJkxgwYIFW/cdO3Ys69evZ+3atRx55JH85V/+JRMmTOC9730vL730EgDz5s1j8eLFW+svuugipkyZwlFHHcVDDz0EQFtbGyeffDJTpkzh4x//OG9729tYv359L5+Jmobu+UmSes7FP1zFA+ue79Fjjj94Py56/4Tttn/5y19m5cqVLF9e+4bJpUuX8otf/IKVK1du/fWAa6+9lgMOOICXXnqJP/mTP+FDH/oQI0du+30ljzzyCNdffz3XXHMNp512Gt///vf56Ec/+qb3O/DAA7nvvvv42te+xmWXXcY3vvENLr74Yt7znvdw4YUXcuutt24TsL3NmZ8kFWrq1Knb/F7cVVddxdFHH81xxx3HE088wSOPPPKmfcaNG0dTU+35BMcccwxr167t8Ninnnrqm2ruuusu5syp/TLAjBkzGDFiRA+Opnuc+UlSL+tshtabhg0btnV56dKl3HHHHdx9990MHTqUE088scPfmxs8ePDW5QEDBmy97Lm9ugEDBrB582ag9svo/YUzP0kqwL777ssLL7yw3faNGzcyYsQIhg4dykMPPcTPf/7zHu/DO9/5ThYtWgTA7bffzrPPPtvj79Eow0+SCjBy5EiOP/54Jk6cyPnnn/+m9hkzZrB582YmTZrE5z73OY47bru/nr3DLrroIm6//XamTJnCLbfcwujRo9l33317/H0aEf1pGtqV5ubmbGlp6etuSFK3Pfjggxx55JF93Y0+9corrzBgwAAGDhzI3Xffzfz587d+AKcndHSOI2JZZja3r/WenySpV/znf/4np512Gq+//jp7770311xzTZ/1xfCTJPWKI444gl/+8pd93Q3Ae36SpAIZfpKk4hh+kqTiGH6SpOIYfpKkDu2zzz4ArFu3jtmzZ3dYc+KJJ9LVr6BdeeWVbNq0aet6I49I2tUMP0lSpw4++OCtT2zYEe3Dr5FHJO1qhp8kFeDTn/70Ns/z+8IXvsBXv/pVXnzxRaZPn7718UM/+MEP3rTv2rVrmThxIgAvvfQSc+bMYdKkSZx++unbfLfn/PnzaW5uZsKECVx00UVA7cuy161bx7Rp05g2bRrw+0ckAVx++eVMnDiRiRMncuWVV259v+09Oqmn+Ht+ktTbbrkAfnt/zx7zD4+CmV/ebvOcOXP45Cc/ySc+8QkAFi1axK233sqQIUO46aab2G+//Vi/fj3HHXccH/jAB4iIDo/z9a9/naFDh7JixQpWrFjBlClTtrb9wz/8AwcccABbtmxh+vTprFixgr/+67/m8ssv58477+TAAw/c5ljLli3jW9/6Fvfccw+ZybHHHsu73/1uRowY0fCjk3aUMz9JKsDkyZN5+umnWbduHb/61a8YMWIEb33rW8lM/v7v/55JkyZx0kkn8Zvf/Iannnpqu8f56U9/ujWEJk2axKRJk7a2LVq0iClTpjB58mRWrVrFAw880Gmf7rrrLj74wQ8ybNgw9tlnH0499VR+9rOfAY0/OmlHOfOTpN7WyQxtV5o9ezaLFy/mt7/97dbn6i1cuJC2tjaWLVvGoEGDGDt2bIePMqrX0azwscce47LLLuPee+9lxIgRzJs3r8vjdPbd0o0+OmlHOfOTpELMmTOHG264gcWLF2/99ObGjRs56KCDGDRoEHfeeSePP/54p8c44YQTWLhwIQArV65kxYoVADz//PMMGzaM4cOH89RTT3HLLbds3Wd7j1M64YQT+Ld/+zc2bdrE7373O2666Sbe9a539dRwO+XMT5IKMWHCBF544QUOOeQQRo8eDcAZZ5zB+9//fpqbm2lqauLtb397p8eYP38+Z599NpMmTaKpqYmpU6cCcPTRRzN58mQmTJjAYYcdxvHHH791n3PPPZeZM2cyevRo7rzzzq3bp0yZwrx587Ye45xzzmHy5Mk9fomzIz7SSJJ6gY802vW680gjL3tKkopj+EmSimP4SVIv2Z1uM+1uuntuDT9J6gVDhgxhw4YNBuAukJls2LCBIUOGNLyPn/aUpF4wZswYWltbaWtr6+uu7JGGDBnCmDFjGq5vKPwiYgbwT8AA4BuZ+eV27YOBbwPHABuA0zNzbUScAZxfVzoJmAL8Gvge8EfAFuCHmXlBw72WpN3MoEGDGDduXF93Q5UuL3tGxADgamAmMB6YGxHj25V9DHg2Mw8HrgAuBcjMhZnZlJlNwJnA2sxcXu1zWWa+HZgMHB8RM3tkRJIkdaGRe35TgdWZuSYzXwVuAGa1q5kFXFctLwamx5u//2YucD1AZm7KzDur5VeB+4DG56uSJO2ERsLvEOCJuvXWaluHNZm5GdgIjGxXczpV+NWLiP2B9wM/6ujNI+LciGiJiBavlUuSekIj4dfRcy3af1yp05qIOBbYlJkrt9kpYiC1QLwqM9d09OaZuSAzmzOzedSoUQ10V5KkzjUSfq3AoXXrY4B126upAm048Exd+xw6mPUBC4BHMvPKRjssSdLOaiT87gWOiIhxEbE3tSBb0q5mCXBWtTwb+HFWv8wSEXsBH6Z2r3CriPgStZD85I53X5Kk7usy/Kp7eOcBtwEPAosyc1VEXBIRH6jKvgmMjIjVwKeA+l9bOAForb+sGRFjgM9Q+/TofRGxPCLO6ZERSZLUBZ/qIEnaY/lUB0mSKoafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOIafJKk4hp8kqTiGnySpOA2FX0TMiIiHI2J1RFzQQfvgiLixar8nIsZW28+IiOV1r9cjoqlqOyYi7q/2uSoioicHJknS9nQZfhExALgamAmMB+ZGxPh2ZR8Dns3Mw4ErgEsBMnNhZjZlZhNwJrA2M5dX+3wdOBc4onrN6IHxSJLUpUZmflOB1Zm5JjNfBW4AZrWrmQVcVy0vBqZ3MJObC1wPEBGjgf0y8+7MTODbwCk7OAZJkrqlkfA7BHiibr212tZhTWZuBjYCI9vVnE4VflV9axfHBCAizo2IlohoaWtra6C7kiR1rpHw6+heXHanJiKOBTZl5spuHLO2MXNBZjZnZvOoUaMa6K4kSZ1rJPxagUPr1scA67ZXExEDgeHAM3Xtc/j9rO+N+jFdHFOSpF2ikfC7FzgiIsZFxN7UgmxJu5olwFnV8mzgx9W9PCJiL+DD1O4VApCZTwIvRMRx1b3BPwd+sFMjkSSpQQO7KsjMzRFxHnAbMAC4NjNXRcQlQEtmLgG+CXwnIlZTm/HNqTvECUBrZq5pd+j5wP8C/gC4pXpJkrTLRTVB2y00NzdnS0tLX3dDkrSbiIhlmdncfrvf8CJJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4SdJKk5D4RcRMyLi4YhYHREXdNA+OCJurNrviYixdW2TIuLuiFgVEfdHxJBq+9xqfUVE3BoRB/bUoCRJ6kyX4RcRA4CrgZnAeGBuRIxvV/Yx4NnMPBy4Ari02ncg8F3grzJzAnAi8Fq1/Z+AaZk5CVgBnNcjI5IkqQuNzPymAqszc01mvgrcAMxqVzMLuK5aXgxMj4gA3gusyMxfAWTmhszcAkT1GlbV7Qes2+nRSJLUgEbC7xDgibr11mpbhzWZuRnYCIwE/hjIiLgtIu6LiL+ral4D5gP3Uwu98cA3d2IckiQ1rJHwiw62ZYM1A4F3AmdUPz8YEdMjYhC18JsMHEztsueFHb55xLkR0RIRLW1tbQ10V5KkzjUSfq3AoXXrY3jzJcqtNdX9vOHAM9X2n2Tm+szcBNwMTAGaADLz0cxMYBHwjo7ePDMXZGZzZjaPGjWq4YFJkrQ9jYTfvcARETEuIvYG5gBL2tUsAc6qlmcDP65C7TZgUkQMrULx3cADwG+A8RHxRpqdDDy4c0ORJKkxA7sqyMzNEXEetSAbAFybmasi4hKgJTOXULtf952IWE1txjen2vfZiLicWoAmcHNm/jtARFwM/DQiXgMeB+b1+OgkSepA1CZou4fm5uZsaWnp625IknYTEbEsM5vbb/cbXiRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScUx/CRJxTH8JEnFMfwkScWJzOzrPjQsItqAx/u6H7vAgcD6vu5EP+b56ZrnqHOen87tyefnbZk5qv3G3Sr89lQR0ZKZzX3dj/7K89M1z1HnPD+dK/H8eNlTklQcw0+SVBzDr39Y0Ncd6Oc8P13zHHXO89O54s6P9/wkScVx5idJKo7h10si4oCI+I+IeKT6OWI7dWdVNY9ExFkdtC+JiJW7vse9a2fOT0QMjYh/j4iHImJVRHy5d3u/60TEjIh4OCJWR8QFHbQPjogbq/Z7ImJsXduF1faHI+LPerPfvWlHz1FEnBwRyyLi/urne3q7771hZ/4MVe1vjYgXI+Jve6vPvSIzffXCC/gKcEG1fAFwaQc1BwBrqp8jquURde2nAv8bWNnX4+lP5wcYCkyravYGfgbM7Osx9cA5GQA8ChxWjetXwPh2NZ8A/qVangPcWC2Pr+oHA+Oq4wzo6zH1s3M0GTi4Wp4I/Kavx9Ofzk9d+/eB7wF/29fj6cmXM7/eMwu4rlq+Djilg5o/A/4jM5/JzGeB/wBmAETEPsCngC/1Ql/7wg6fn8zclJl3AmTmq8B9wJhe6POuNhVYnZlrqnHdQO081as/b4uB6RER1fYbMvOVzHwMWF0db0+zw+coM3+Zmeuq7auAIRExuFd63Xt25s8QEXEKtX9kruql/vYaw6/3vCUznwSofh7UQc0hwBN1663VNoAvAl8FNu3KTvahnT0/AETE/sD7gR/ton72pi7HW1+TmZuBjcDIBvfdE+zMOar3IeCXmfnKLupnX9nh8xMRw4BPAxf3Qj973cC+7sCeJCLuAP6wg6bPNHqIDrZlRDQBh2fm37S/Hr872VXnp+74A4Hrgasyc033e9jvdDreLmoa2XdPsDPnqNYYMQG4FHhvD/arv9iZ83MxcEVmvlhNBPcohl8PysyTttcWEU9FxOjMfDIiRgNPd1DWCpxYtz4GWAr8KXBMRKyl9t/soIhYmpknshvZhefnDQuARzLzyh7obn/QChxatz4GWLedmtYq/IcDzzS4755gZ84RETEGuAn488x8dNd3t9ftzPk5FpgdEV8B9gdej4iXM/Ofd323e0Ff33Qs5QX8T7b9QMdXOqg5AHiM2oc4RlTLB7SrGcue+YGXnTo/1O6Ffh/Yq6/H0oPnZCC1+y3j+P2HFSa0q/mvbPthhUXV8gS2/cDLGvbMD7zszDnav6r/UF+Poz+en3Y1X2AP+8BLn3eglBe1eww/Ah6pfr7xP+1m4Bt1dX9B7cMJq4GzOzjOnhp+O3x+qP1rNoEHgeXV65y+HlMPnZf3Ab+m9om9z1TbLgE+UC0PofZJvNXAL4DD6vb9TLXfw+wBn37t6XMEfBb4Xd2fmeXAQX09nv5yftodY48LP7/hRZJUHD/tKUkqjuEnSSqO4SdJKo7hJ0kqjuEnSSqO4ScVICJOjIj/29f9kPoLw0+SVBzDT+pHIuKjEfGLiFgeEf8aEQOqZ6l9NSLui4gfRcSoqrYpIn4eESsi4qY3noEYEYdHxB0R8atqnz+qDr9PRCyunnu48I1v7pdKZPhJ/UREHAmcDhyfmU3AFuAMYBhwX2ZOAX4CXFTt8m3g05k5Cbi/bvtC4OrMPBp4B/BktX0y8Elqz/o7DDh+lw9K6qf8Ymup/5gOHAPcW03K/oDaF3y/DtxY1XwX+D8RMRzYPzN/Um2/DvheROwLHJKZNwFk5ssA1fF+kZmt1fpyal+Vd9euH5bU/xh+Uv8RwHWZeeE2GyM+166us+8k7OxSZv2z6rbg338VzMueUv/xI2qPkDkIICIOiIi3Uft7Oruq+QhwV2ZuBJ6NiHdV288EfpKZz1N7NM0p1TEGR8TQXh2FtBvwX35SP5GZD0TEZ4HbI2Iv4DVqj5v5HTAhIpZRe8r26dUuZwH/UoXbGuDsavuZwL9GxCXVMT7ci8OQdgs+1UHq5yLixczcp6/7Ie1JvOwpSSqOMz9JUnGc+UmSimP4SZKKY/hJkopj+EmSimP4SZKKY/hJkorz/wH6QRJHLsjMWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss\n",
      "\ttraining         \t (min:    0.079, max:    0.079, cur:    0.079)\n",
      "\tvalidation       \t (min:    0.067, max:    0.067, cur:    0.067)\n",
      "\n",
      "[1/16]    {'log loss': 0.07912648469209671, 'val_log loss': 0.06703554838895798}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-37adb9d2a140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m# _ stands in for labels, here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# no need to flatten images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 16\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for epoch_id, epoch in enumerate(range(1, n_epochs+1)):\n",
    "    # monitor training loss\n",
    "    logs = {}\n",
    "\n",
    "    for phase in ['train', 'test']:\n",
    "        running_loss = 0.0\n",
    "        for data in dataloaders[phase]:\n",
    "            # _ stands in for labels, here\n",
    "            # no need to flatten images\n",
    "            images, _ = data\n",
    "            images = images.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, images)\n",
    "            if phase == 'train':\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # update running training loss\n",
    "            running_loss += loss.detach() * images.size(0)\n",
    "            \n",
    "        # print avg training statistics \n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        if phase == 'test':\n",
    "            prefix = 'val_' # that shit is hardcoded in PlotLosses - test called as \"validation\"\n",
    "        if phase == 'train':\n",
    "            prefix = ''     # that one too - \"train\" will be displayed only if label is empty\n",
    "        logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "    print(f\"\\n[{epoch_id+1}/{n_epochs}]    {logs}\")\n",
    "print(\"\\nTrained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "dataiter.next()\n",
    "dataiter.next()\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "output = output.cpu()\n",
    "# prep images for display\n",
    "images = images.cpu()\n",
    "images = images.numpy()\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray', vmin=0, vmax=1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### random input\n",
    "random_input = np.empty((10, 1, 28, 28))\n",
    "for j in range(0, 10):\n",
    "    random_input[j][0] = (np.random.randint(0, 100, (28*28))/100.0).reshape(28, 28)\n",
    "    \n",
    "output = model(torch.Tensor(random_input).to(device))\n",
    "output = output.cpu()\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([random_input, output], axes):\n",
    "    for img, ax in zip(random_input, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray', vmin=0, vmax=1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"diagonal\" latent layer\n",
    "decode_input = np.empty((10, 64, 1, 1))\n",
    "for j in range(0, 10):\n",
    "    fill = (j)/10.0\n",
    "    decode_input[int(j)] = np.full((64, 1, 1), fill)\n",
    "\n",
    "output = model.decode(torch.Tensor(decode_input).to(device))\n",
    "output = output.cpu()\n",
    "output = output.detach().numpy()\n",
    "\n",
    "decode_images = np.empty((64, 4, 4))\n",
    "for index, j in enumerate(decode_input):\n",
    "    decode_images[int(index)] = j.sum(axis=0)/10.0\n",
    "    \n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(decode_images, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### latent: 1 of 16 dimensions filled with 1\n",
    "# decode_input = np.empty((10, 16, 7, 7))\n",
    "# for j in range(0, 10):\n",
    "#     fill = (j)/10.0\n",
    "#     decode_input[int(j)] = np.full((16, 7, 7), fill)\n",
    "#     for jj in range(0, 1):\n",
    "#         decode_input[int(j)][jj] = np.full((7, 7), 1)\n",
    "\n",
    "# output = model.decode(torch.Tensor(decode_input).to(device))\n",
    "# output = output.cpu()\n",
    "# output = output.detach().numpy()\n",
    "\n",
    "# decode_images = np.empty((10, 7, 7))\n",
    "# for index, j in enumerate(decode_input):\n",
    "#     decode_images[int(index)] = j.sum(axis=0)/16.0\n",
    "    \n",
    "# # plot the first ten input images and then reconstructed images\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# # input images on top row, reconstructions on bottom\n",
    "# for images, row in zip(decode_images, axes):\n",
    "#     row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# # input images on top row, reconstructions on bottom\n",
    "# for images, row in zip(output, axes):\n",
    "#     row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "# fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# # input images on top row, reconstructions on bottom\n",
    "# for images, row in zip(output, axes):\n",
    "#     row.imshow(np.squeeze(images), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_input = np.empty((10, 64, 1, 1))\n",
    "for j in range(0, 10):\n",
    "    fill = 0\n",
    "    decode_input[int(j)] = np.full((64, 1, 1), fill)\n",
    "    decode_input[j][j][0][0] = 1.0\n",
    "\n",
    "output = model.decode(torch.Tensor(decode_input).to(device))\n",
    "output = output.cpu()\n",
    "output = output.detach().numpy()\n",
    "\n",
    "#print(decode_input.reshape(10,10))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_input = np.empty((10, 64, 1, 1))\n",
    "for j in range(0, 10):\n",
    "    fill = 0\n",
    "    decode_input[int(j)] = np.full((64, 1, 1), fill)\n",
    "\n",
    "decode_input[0][1][0][0] = 1.0\n",
    "decode_input[0][8][0][0] = 1.0\n",
    "decode_input[0][9][0][0] = 1.0\n",
    "    \n",
    "output = model.decode(torch.Tensor(decode_input).to(device))\n",
    "output = output.cpu()\n",
    "output = output.detach().numpy()\n",
    "\n",
    "#print(decode_input.reshape(10,10))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,5))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip(output, axes):\n",
    "    row.imshow(np.squeeze(images), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
